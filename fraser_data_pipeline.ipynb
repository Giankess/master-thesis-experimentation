{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fa92833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# data collection\n",
    "import feedparser\n",
    "import newspaper\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# data processing\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS  # For storing and retrieving embeddings using the FAISS library\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib # For saving models\n",
    "\n",
    "#Cluster Classifier & Interpretation\n",
    "import shap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da73f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 305 events.\n",
      "\n",
      "--- First 5 Scraped Events ---\n",
      "| Date              | Source                                        | Summary                                                                                                                                                          | Associated Link Title                                                                                                                                                                                                                  | Associated Link URL                                                              |\n",
      "|:------------------|:----------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|\n",
      "| February 27, 2007 | Freddie Mac Press Release                     | The Federal Home Loan Mortgage Corporation (Freddie Mac) announces that it will no longer buy the most risky subprime mortgages and mortgage-related securities. | Freddie Mac Announces Tougher Subprime Lending Standards to Help Reduce the Risk of Future Borrower Default: Company Also to Develop Model Subprime Mortgages                                                                          | https://fraser.stlouisfed.org/title/5132/item/518857                             |\n",
      "| April 2, 2007     | SEC Filing: New Century Financial Corporation | New Century Financial Corporation, a leading subprime mortgage lender, files for Chapter 11 bankruptcy protection.                                               | New Century Financial Corporation: Form 8-K and Documentation, Including Press Release                                                                                                                                                 | https://fraser.stlouisfed.org/archival/5147/item/519318                          |\n",
      "| June 1, 2007      | Congressional Testimony                       | Standard and Poor's and Moody's Investor Services downgrade over 100 bonds backed by second-lien subprime mortgages.                                             | Testimony of Vickie A. Tillman, Executive Vice President, Standard & Poor's Credit Market Services: Before the Subcommittee on Capital Markets, Insurance and Government Sponsored Enterprises, United States House of Representatives | https://fraser.stlouisfed.org/title/5073                                         |\n",
      "| June 7, 2007      | Bear Stearns Suspends Redemptions             | Bear Stearns informs investors that it is suspending redemptions from its High-Grade Structured Credit Strategies Enhanced Leverage Fund.                        | N/A                                                                                                                                                                                                                                    | N/A                                                                              |\n",
      "| June 28, 2007     | Federal Reserve Press Release                 | The Federal Open Market Committee (FOMC) votes to maintain its target for the federal funds rate at 5.25 percent.                                                | Meeting, June 27-28, 2007: Statement                                                                                                                                                                                                   | https://fraser.stlouisfed.org/title/677/item/23456/content/pdf/20070618statement |\n"
     ]
    }
   ],
   "source": [
    "# Fraser Data Collection\n",
    "\n",
    "def scrape_financial_crisis_timeline(url):\n",
    "    \"\"\"Scrapes date, title, description, and link info from the timeline.\"\"\"\n",
    "    try:\n",
    "        # 1. Download the HTML content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "        # 2. Parse the HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 3. Find the main container element\n",
    "        # Your target container is class=\"timeline-events clusterize-scroll\" and id=\"list-container\"\n",
    "        timeline_container = soup.find('div', id='list-container')\n",
    "        \n",
    "        if not timeline_container:\n",
    "            print(\"Error: Main timeline container not found.\")\n",
    "            return []\n",
    "\n",
    "        # 4. Find all individual event rows\n",
    "        # The articles are inside <div class=\"row event-row active\">\n",
    "        event_rows = timeline_container.find_all('div', class_='event-row')\n",
    "\n",
    "        data = []\n",
    "        for row in event_rows:\n",
    "            # 5. Extract data points using the specific classes\n",
    "            \n",
    "            # Date and Source/Title: <h2 class=\"list-item\">\n",
    "            header_element = row.find('h2', class_='list-item')\n",
    "            header_text = header_element.text.strip() if header_element else 'N/A'\n",
    "            \n",
    "            # Description/Summary: <p class=\"list-item\">\n",
    "            summary_element = row.find('p', class_='list-item')\n",
    "            summary = summary_element.text.strip() if summary_element else 'N/A'\n",
    "            \n",
    "            # Associated Link: <ul><li><a href=\"...\" class=\"list-item\">\n",
    "            link_element = row.find('a', class_='list-item')\n",
    "            \n",
    "            link_title = link_element.text.strip() if link_element else 'N/A'\n",
    "            link_url = link_element['href'] if link_element else 'N/A'\n",
    "            \n",
    "            # Prepend the base URL if the link is relative\n",
    "            if link_url != 'N/A' and link_url.startswith('/'):\n",
    "                link_url = 'https://fraser.stlouisfed.org' + link_url\n",
    "            \n",
    "            # Split the header into Date and Source\n",
    "            if '|' in header_text:\n",
    "                date, source = [part.strip() for part in header_text.split('|', 1)]\n",
    "            else:\n",
    "                date = header_text\n",
    "                source = 'N/A'\n",
    "\n",
    "            data.append({\n",
    "                'Date': date,\n",
    "                'Source': source,\n",
    "                'Summary': summary,\n",
    "                'Associated Link Title': link_title,\n",
    "                'Associated Link URL': link_url\n",
    "            })\n",
    "\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Execution ---\n",
    "URL = \"https://fraser.stlouisfed.org/timeline/financial-crisis\"\n",
    "fraser_timeline_data = scrape_financial_crisis_timeline(URL)\n",
    "\n",
    "# Output the results (first 5 entries)\n",
    "if fraser_timeline_data:\n",
    "    df = pd.DataFrame(fraser_timeline_data)\n",
    "    print(f\"Scraped {len(df)} events.\")\n",
    "    print(\"\\n--- First 5 Scraped Events ---\")\n",
    "    print(df.head().to_markdown(index=False))\n",
    "else:\n",
    "    print(\"Failed to scrape data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e7d3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating FinBERT Embeddings on cpu ---\n",
      "\n",
      "--- Embeddings Generated ---\n",
      "Shape of Embeddings: (305, 768)\n",
      "DataFrame with embeddings ready for PCA/Clustering.\n"
     ]
    }
   ],
   "source": [
    "# A specific model for generating embeddings, distinct from the sentiment classifier\n",
    "EMBEDDING_MODEL_NAME = \"ProsusAI/finbert\" \n",
    "\n",
    "def get_finbert_sentence_embeddings(texts, model_name=EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Loads FinBERT, tokenizes texts, and extracts the [CLS] token embedding \n",
    "    as the sentence representation.\n",
    "    \"\"\"\n",
    "    # 1. Load Tokenizer and Model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Using AutoModel for embeddings (not AutoModelForSequenceClassification, which is for sentiment classification)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Check for GPU and move model if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\n--- Generating FinBERT Embeddings on {device} ---\")\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # 2. Tokenize the text\n",
    "        # BERT models have a max sequence length (typically 512). \n",
    "        # The 'full_text' needs truncation or splitting (advanced step).\n",
    "        inputs = tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 3. Get model outputs\n",
    "            outputs = model(**inputs)\n",
    "            # The last hidden state contains the final embeddings for all tokens\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            \n",
    "            # 4. Extract the [CLS] token vector as the sentence embedding\n",
    "            # [CLS] token is at index 0, and we squeeze to remove the batch dimension\n",
    "            cls_embedding = last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(cls_embedding)\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "if not df.empty:\n",
    "    # Use the full_text column from the DataFrame created in Step 1\n",
    "    texts_to_embed = df['Summary'].tolist()\n",
    "\n",
    "    # Get the 768-dimensional embeddings\n",
    "    finbert_embeddings = get_finbert_sentence_embeddings(texts_to_embed)\n",
    "\n",
    "    # Add embeddings to the DataFrame for the next pipeline step\n",
    "    df['finbert_embedding'] = list(finbert_embeddings)\n",
    "\n",
    "    print(\"\\n--- Embeddings Generated ---\")\n",
    "    print(f\"Shape of Embeddings: {finbert_embeddings.shape}\")\n",
    "    print(\"DataFrame with embeddings ready for PCA/Clustering.\")\n",
    "else:\n",
    "    print(\"No articles to process. Please check data acquisition step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db6e3a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Embeddings Shape: (305, 768)\n",
      "PCA Reduced Data Shape: (305, 100)\n",
      "[-14.84868      6.1126013    3.4305367    5.76674     -9.525345\n",
      "  -1.1116639    5.3272      -7.2235284   -0.02011316   1.2409043\n",
      "   3.5410328   -0.6319603   -9.017167    -1.624022    -1.6866331\n",
      "   0.5924423   -1.8345335   -0.09928001   0.26945448   7.2866435\n",
      "  -0.7791316    1.4217315    7.994451     0.43119586  -1.099438\n",
      "  -3.9576192    6.4014726    5.5783834    3.2479494    0.608338\n",
      "   2.9499784    3.0143456   -0.07458016   4.8942504    0.07216936\n",
      "   2.028043    -1.4730884   -0.59757      1.3931813    1.0009661\n",
      "  -0.34108952   1.0862185   -1.6466125    1.3988231    0.7174072\n",
      "   0.3173057    0.4327445    0.42597026  -0.9464779   -0.84037995\n",
      "  -0.5054373   -2.5273292    0.693739     1.0381192   -0.58131975\n",
      "   1.1044999    0.7060808   -1.6274889   -0.36401194  -0.5489241\n",
      "   1.8548692    2.7607353    4.1266346    1.2411493    0.5907885\n",
      "  -1.4627923    1.5553414    1.3721336   -1.4760609   -1.0088372\n",
      "  -1.3397287   -0.274771     2.6390128   -0.21205041  -0.8037345\n",
      "   0.07846699  -1.4185033    0.3934929    0.61918044  -0.83742946\n",
      "   1.9830091   -1.4817185   -0.22682668   0.74581397   1.6420567\n",
      "  -0.25580826  -0.31675428  -0.7590418    2.0435898   -0.06807962\n",
      "   0.8045616    2.5582755    1.085358    -0.6511259   -0.4675944\n",
      "   1.7650553   -1.2894257   -0.8622725   -0.0496377   -1.3646395 ]\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "embeddings = np.array(df['finbert_embedding'].tolist())\n",
    "n_components = 100  # Target reduced dimension\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "print(f\"Scaled Embeddings Shape: {scaled_embeddings.shape}\")\n",
    "\n",
    "# 2. Dimensionality Reduction (PCA)\n",
    "# Target: Reduce from 768-dim to a more manageable size (e.g., 100)\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "reduced_data = pca.fit_transform(scaled_embeddings)\n",
    "print(f\"PCA Reduced Data Shape: {reduced_data.shape}\")\n",
    "print(reduced_data[0])  # Print the first reduced vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0da8e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2, the average silhouette_score is : 0.27166562223966845\n",
      "For n_clusters = 3, the average silhouette_score is : 0.19192472920384226\n",
      "For n_clusters = 4, the average silhouette_score is : 0.183505553280819\n",
      "For n_clusters = 5, the average silhouette_score is : 0.14040711896718577\n",
      "For n_clusters = 6, the average silhouette_score is : 0.15328536003952573\n",
      "For n_clusters = 7, the average silhouette_score is : 0.14642039211012797\n",
      "For n_clusters = 8, the average silhouette_score is : 0.1524589730811093\n",
      "For n_clusters = 9, the average silhouette_score is : 0.14855807291520234\n",
      "For n_clusters = 10, the average silhouette_score is : 0.14635362145389713\n",
      "\n",
      "Best number of clusters by silhouette score: 2 with a score of 0.27166562223966845\n"
     ]
    }
   ],
   "source": [
    "# Get optimal amount of clusters using silhouette score\n",
    "reduced_data = reduced_data.astype(np.float64)\n",
    "range_n_clusters = list(range(2, 11))\n",
    "best_n_clusters = 0\n",
    "best_silhouette_score = -1\n",
    "for n_clusters in range_n_clusters:\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=42, reg_covar=1e-5)\n",
    "    cluster_labels = gmm.fit_predict(reduced_data)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")\n",
    "    \n",
    "    if silhouette_avg > best_silhouette_score:\n",
    "        best_silhouette_score = silhouette_avg\n",
    "        best_n_clusters = n_clusters\n",
    "\n",
    "print(f\"\\nBest number of clusters by silhouette score: {best_n_clusters} with a score of {best_silhouette_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe7c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Clustering Completed ---\n",
      "                Date                                         Source  Cluster\n",
      "0  February 27, 2007                      Freddie Mac Press Release        0\n",
      "1      April 2, 2007  SEC Filing: New Century Financial Corporation        0\n",
      "2       June 1, 2007                        Congressional Testimony        0\n",
      "3       June 7, 2007              Bear Stearns Suspends Redemptions        0\n",
      "4      June 28, 2007                  Federal Reserve Press Release        0\n"
     ]
    }
   ],
   "source": [
    "# Clustering with the best number of clusters\n",
    "n_clusters = best_n_clusters\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42, covariance_type='full')\n",
    "gmm_labels = gmm.fit_predict(reduced_data)\n",
    "df['Cluster'] = gmm_labels\n",
    "print(\"\\n--- Clustering Completed ---\")\n",
    "print(df[['Date', 'Source', 'Cluster']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca145e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_clusters_with_shap(reduced_data,\n",
    "                                 cluster_labels,\n",
    "                                 df,\n",
    "                                 pca_model,\n",
    "                                 scaler=None,\n",
    "                                 top_pcs=10,\n",
    "                                 top_embeddings=20,\n",
    "                                 n_prototypes=3,\n",
    "                                 shap_sample=200,\n",
    "                                 random_state=42):\n",
    "    \"\"\"\n",
    "    Improved cluster interpretation pipeline.\n",
    "    Args:\n",
    "      reduced_data: ndarray (n_samples, n_pca_components) -- PCA-transformed data used for clustering\n",
    "      cluster_labels: ndarray (n_samples,) -- cluster ids from GMM/other\n",
    "      df: pandas.DataFrame -- original dataframe containing text fields ('Summary' or 'full_text', 'title' optional)\n",
    "      pca_model: fitted sklearn PCA instance used to produce reduced_data\n",
    "      scaler: optional fitted scaler applied before PCA (if used). If not None, used for reverse mapping comments.\n",
    "      top_pcs: number of top PCA components to report per cluster\n",
    "      top_embeddings: number of original embedding dimensions to surface when mapping back\n",
    "      n_prototypes: number of prototype documents to show per cluster\n",
    "      shap_sample: max number of rows used by SHAP explainer for speed (Kernel explainer avoided)\n",
    "      Returns:\n",
    "        results dict with per-cluster SHAP importances, mapped original-dim importances,\n",
    "        prototype indices and keywords.\n",
    "    Notes:\n",
    "      - This function trains a RandomForest surrogate (works well with TreeExplainer for SHAP).\n",
    "      - Reverse mapping from PCA -> original embedding dims is approximate and shows which\n",
    "        embedding-dimensions (e.g., FinBERT hidden dims) matter most. Mapping embedding dims\n",
    "        to tokens requires further steps (token-level activations / nearest-neighbor in token embedding space).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Validate inputs\n",
    "    n_samples, n_pcs = reduced_data.shape\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    n_clusters = unique_clusters.size\n",
    "    assert len(cluster_labels) == n_samples, \"cluster_labels length mismatch\"\n",
    "\n",
    "    # 1) Train surrogate classifier (Random Forest) to predict cluster from PCA features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        reduced_data, cluster_labels, test_size=0.2, random_state=random_state, stratify=cluster_labels\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=random_state, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nSurrogate classifier trained. Test accuracy: {:.3f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "    # 2) SHAP explanation using TreeExplainer (fast for tree models)\n",
    "    # Sample background for SHAP if dataset is large\n",
    "    background = X_train\n",
    "    if background.shape[0] > shap_sample:\n",
    "        idx = np.random.choice(background.shape[0], shap_sample, replace=False)\n",
    "        background = background[idx]\n",
    "\n",
    "    explainer = shap.TreeExplainer(clf, data=background, model_output='probability')\n",
    "    # Use a reasonable test subset for SHAP values (speed)\n",
    "    shap_X = X_test\n",
    "    if shap_X.shape[0] > shap_sample:\n",
    "        idx = np.random.choice(shap_X.shape[0], shap_sample, replace=False)\n",
    "        shap_X = shap_X[idx]\n",
    "\n",
    "    shap_values = explainer.shap_values(shap_X)  # list length = n_classes, each (m, n_pcs)\n",
    "\n",
    "    results = {\n",
    "        'n_clusters': n_clusters,\n",
    "        'cluster_summaries': {}\n",
    "    }\n",
    "\n",
    "    # Prepare TF-IDF for cluster keywords / prototype doc keywords\n",
    "    text_field = None\n",
    "    for candidate in ['Summary', 'full_text', 'text']:\n",
    "        if candidate in df.columns:\n",
    "            text_field = candidate\n",
    "            break\n",
    "    if text_field is None:\n",
    "        raise ValueError(\"No text column found in df. Expected one of ['Summary','full_text','text'].\")\n",
    "\n",
    "    corpus = df[text_field].fillna(\"\").astype(str).tolist()\n",
    "    tfidf = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1,2))\n",
    "    tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "    feature_names = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "    # cluster centroids in reduced space (use mean of members)\n",
    "    centroids = np.vstack([reduced_data[cluster_labels == c].mean(axis=0) for c in unique_clusters])\n",
    "\n",
    "    # 3) For each cluster summarize SHAP importances (per PCA component) and map back to original embedding dims\n",
    "    for class_idx, cluster_id in enumerate(unique_clusters):\n",
    "        # shap_values[class_idx] shape (m, n_pcs)\n",
    "        class_shap = shap_values[class_idx]  # sample x n_pcs\n",
    "        mean_abs_shap = np.mean(np.abs(class_shap), axis=0)  # shape (n_pcs,)\n",
    "\n",
    "        # Top PCA components for this cluster\n",
    "        top_pc_idx = np.argsort(mean_abs_shap)[::-1][:top_pcs]\n",
    "        top_pc_scores = mean_abs_shap[top_pc_idx]\n",
    "\n",
    "        # Map PCA importance -> original embedding dims (approximate)\n",
    "        # pca.components_ shape: (n_pcs, original_dim)\n",
    "        # To get per-original-dim importance: aggregate absolute contribution across selected PCs\n",
    "        # Use weighted sum of absolute PCA loadings by mean_abs_shap\n",
    "        # result shape: (original_dim,)\n",
    "        pca_components = pca_model.components_  # (n_pcs, orig_dim)\n",
    "        # Weighted contribution from all PCs using the mean_abs_shap\n",
    "        contribution_orig = np.abs((mean_abs_shap[:, None] * np.abs(pca_components))).sum(axis=0)\n",
    "        top_embed_idx = np.argsort(contribution_orig)[::-1][:top_embeddings]\n",
    "        top_embed_scores = contribution_orig[top_embed_idx]\n",
    "\n",
    "        # 4) Prototype documents: nearest to centroid\n",
    "        members_idx = np.where(cluster_labels == cluster_id)[0]\n",
    "        if members_idx.size == 0:\n",
    "            prototypes = []\n",
    "        else:\n",
    "            member_coords = reduced_data[members_idx]\n",
    "            centroid = centroids[class_idx]\n",
    "            dists = np.linalg.norm(member_coords - centroid[None, :], axis=1)\n",
    "            nearest = np.argsort(dists)[:n_prototypes]\n",
    "            prototypes = members_idx[nearest].tolist()\n",
    "\n",
    "        # 5) Top TF-IDF keywords for the cluster (aggregate TF-IDF across members)\n",
    "        if members_idx.size > 0:\n",
    "            cluster_tfidf_sum = np.asarray(tfidf_matrix[members_idx].sum(axis=0)).ravel()\n",
    "            top_terms_idx = np.argsort(cluster_tfidf_sum)[::-1][:20]\n",
    "            top_terms = feature_names[top_terms_idx]\n",
    "        else:\n",
    "            top_terms = []\n",
    "\n",
    "        # Store results\n",
    "        results['cluster_summaries'][int(cluster_id)] = {\n",
    "            'top_pca_components_idx': top_pc_idx.tolist(),\n",
    "            'top_pca_component_scores': top_pc_scores.tolist(),\n",
    "            'top_embedding_dims_idx': top_embed_idx.tolist(),\n",
    "            'top_embedding_dim_scores': top_embed_scores.tolist(),\n",
    "            'prototype_doc_indices': prototypes,\n",
    "            'top_tfidf_terms': top_terms.tolist()\n",
    "        }\n",
    "\n",
    "        # Print short human-friendly summary\n",
    "        print(f\"\\nCluster {cluster_id} — top PCA components: {top_pc_idx.tolist()} (scores ~ {top_pc_scores.round(4).tolist()})\")\n",
    "        print(f\"Cluster {cluster_id} — prototype doc indices: {prototypes}\")\n",
    "        print(f\"Cluster {cluster_id} — top TF-IDF terms: {top_terms[:10].tolist()}\")\n",
    "\n",
    "        # Optional: SHAP summary plot for this class (requires display)\n",
    "        try:\n",
    "            shap.summary_plot(class_shap, shap_X, feature_names=[f\"PC{i+1}\" for i in range(n_pcs)], show=True)\n",
    "        except Exception:\n",
    "            # In headless environments plotting may fail; ignore\n",
    "            pass\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
