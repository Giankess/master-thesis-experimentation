{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cde81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# data collection\n",
    "import feedparser\n",
    "import newspaper\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FED Data Collection\n",
    "\n",
    "# Set a higher recursion limit for newspaper3k when scraping many articles\n",
    "# This helps prevent recursion errors on complex web pages.\n",
    "sys.setrecursionlimit(2000)\n",
    "\n",
    "rss_url = \"https://www.federalreserve.gov/feeds/press_all.xml\"\n",
    "max_articles = 50  # Limit the number of articles to scrape for testing\n",
    "\n",
    "\n",
    "feed = feedparser.parse(rss_url)\n",
    "articles_data = []\n",
    "    \n",
    "# Iterate through entries, limiting the number of articles for a manageable run\n",
    "for i, entry in enumerate(feed.entries):\n",
    "    if i >= max_articles:\n",
    "        print(f\"Reached max_articles limit of {max_articles}.\")\n",
    "        break\n",
    "            \n",
    "    try:\n",
    "            # 1. Extract link and basic metadata from RSS entry\n",
    "        link = entry.link\n",
    "        title = entry.title\n",
    "            \n",
    "        # Extract date using feedparser's published_parsed\n",
    "        published_date = datetime.fromtimestamp(time.mktime(entry.published_parsed)).isoformat()\n",
    "            \n",
    "        # 2. Use newspaper3k to get the full article text\n",
    "        article = newspaper.Article(link)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "            \n",
    "        # CRUCIAL: Check if full text was successfully extracted\n",
    "        if article.text:\n",
    "            articles_data.append({\n",
    "                'source': 'FED',\n",
    "                'title': title,\n",
    "                    'link': link,\n",
    "                    'date': published_date,\n",
    "                    'full_text': article.text\n",
    "                })\n",
    "        print(f\"[{i+1}/{max_articles}] Successfully scraped: {title[:70]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "    # Skip if an article link is broken or parsing fails\n",
    "        print(f\"[{i+1}/{max_articles}] Error scraping article at {entry.link}: {e}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "fed_df = pd.DataFrame(articles_data)\n",
    "print(fed_df[['title', 'date', 'full_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECB Data Collection\n",
    "# Increase recursion limit to handle potential deep HTML structures\n",
    "sys.setrecursionlimit(2000)\n",
    "\n",
    "# The ECB RSS link providing the data you shared (Press releases, speeches, etc.)\n",
    "ECB_COMBINED_RSS = \"https://www.ecb.europa.eu/rss/press.html\"\n",
    "\n",
    "def scrape_full_articles_from_ecb(rss_url, max_articles=20):\n",
    "    \"\"\"\n",
    "    Parses the ECB combined RSS feed, applies filters, and uses newspaper3k\n",
    "    to extract the full article text from standard HTML links.7\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting scrape for ECB Combined RSS: {rss_url} ---\")\n",
    "    \n",
    "    feed = feedparser.parse(rss_url)\n",
    "    articles_data = []\n",
    "    \n",
    "    for i, entry in enumerate(feed.entries):\n",
    "        if i >= max_articles:\n",
    "            print(f\"Reached max_articles limit of {max_articles}.\")\n",
    "            break\n",
    "            \n",
    "        link = entry.link\n",
    "        \n",
    "        # ⚠️ CRITICAL FILTERING STEP:\n",
    "        # 1. Skip PDF links directly (e.g., Philip R. Lane's contribution)\n",
    "        # 2. Skip internal Decisions (e.g., Decisions taken by the Governing Council) as they are often just short notices.\n",
    "        if link.endswith('.pdf') or '/govcdec/' in link:\n",
    "             print(f\"[{i+1}/{max_articles}] Skipping PDF or Decision link: {entry.title[:40]}...\")\n",
    "             continue\n",
    "        \n",
    "        try:\n",
    "            title = entry.title\n",
    "            \n",
    "            # Safely handle the date parsing\n",
    "            published_date = datetime.fromtimestamp(time.mktime(entry.published_parsed)).isoformat()\n",
    "            \n",
    "            article = newspaper.Article(link)\n",
    "            article.download()\n",
    "            \n",
    "            # A short wait can help with complex sites\n",
    "            time.sleep(0.5) \n",
    "            \n",
    "            article.parse()\n",
    "            \n",
    "            # Ensure the extracted text is substantial (e.g., over 200 characters)\n",
    "            if article.text and len(article.text) > 200: \n",
    "                articles_data.append({\n",
    "                    'source': 'ECB',\n",
    "                    'title': title,\n",
    "                    'link': link,\n",
    "                    'date': published_date,\n",
    "                    'full_text': article.text\n",
    "                })\n",
    "                print(f\"[{i+1}/{max_articles}] Successfully scraped: {title[:70]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch exceptions like ReadTimeout or ParsingError\n",
    "            print(f\"[{i+1}/{max_articles}] Error scraping article at {link}: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(articles_data)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "ecb_df = scrape_full_articles_from_ecb(ECB_COMBINED_RSS, max_articles=15)\n",
    "\n",
    "print(\"\\n--- Verified ECB Scrape Complete ---\")\n",
    "print(f\"Total articles scraped: {len(ecb_df)} (after filtering)\")\n",
    "print(\"\\nDataFrame Head:\")\n",
    "print(ecb_df[['title', 'date', 'full_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34435666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 305 events.\n",
      "\n",
      "--- First 5 Scraped Events ---\n",
      "| Date              | Source                                        | Summary                                                                                                                                                          | Associated Link Title                                                                                                                                                                                                                  | Associated Link URL                                                              |\n",
      "|:------------------|:----------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|\n",
      "| February 27, 2007 | Freddie Mac Press Release                     | The Federal Home Loan Mortgage Corporation (Freddie Mac) announces that it will no longer buy the most risky subprime mortgages and mortgage-related securities. | Freddie Mac Announces Tougher Subprime Lending Standards to Help Reduce the Risk of Future Borrower Default: Company Also to Develop Model Subprime Mortgages                                                                          | https://fraser.stlouisfed.org/title/5132/item/518857                             |\n",
      "| April 2, 2007     | SEC Filing: New Century Financial Corporation | New Century Financial Corporation, a leading subprime mortgage lender, files for Chapter 11 bankruptcy protection.                                               | New Century Financial Corporation: Form 8-K and Documentation, Including Press Release                                                                                                                                                 | https://fraser.stlouisfed.org/archival/5147/item/519318                          |\n",
      "| June 1, 2007      | Congressional Testimony                       | Standard and Poor's and Moody's Investor Services downgrade over 100 bonds backed by second-lien subprime mortgages.                                             | Testimony of Vickie A. Tillman, Executive Vice President, Standard & Poor's Credit Market Services: Before the Subcommittee on Capital Markets, Insurance and Government Sponsored Enterprises, United States House of Representatives | https://fraser.stlouisfed.org/title/5073                                         |\n",
      "| June 7, 2007      | Bear Stearns Suspends Redemptions             | Bear Stearns informs investors that it is suspending redemptions from its High-Grade Structured Credit Strategies Enhanced Leverage Fund.                        | N/A                                                                                                                                                                                                                                    | N/A                                                                              |\n",
      "| June 28, 2007     | Federal Reserve Press Release                 | The Federal Open Market Committee (FOMC) votes to maintain its target for the federal funds rate at 5.25 percent.                                                | Meeting, June 27-28, 2007: Statement                                                                                                                                                                                                   | https://fraser.stlouisfed.org/title/677/item/23456/content/pdf/20070618statement |\n"
     ]
    }
   ],
   "source": [
    "# Fraser Data Collection\n",
    "\n",
    "def scrape_financial_crisis_timeline(url):\n",
    "    \"\"\"Scrapes date, title, description, and link info from the timeline.\"\"\"\n",
    "    try:\n",
    "        # 1. Download the HTML content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "        # 2. Parse the HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 3. Find the main container element\n",
    "        # Your target container is class=\"timeline-events clusterize-scroll\" and id=\"list-container\"\n",
    "        timeline_container = soup.find('div', id='list-container')\n",
    "        \n",
    "        if not timeline_container:\n",
    "            print(\"Error: Main timeline container not found.\")\n",
    "            return []\n",
    "\n",
    "        # 4. Find all individual event rows\n",
    "        # The articles are inside <div class=\"row event-row active\">\n",
    "        event_rows = timeline_container.find_all('div', class_='event-row')\n",
    "\n",
    "        data = []\n",
    "        for row in event_rows:\n",
    "            # 5. Extract data points using the specific classes\n",
    "            \n",
    "            # Date and Source/Title: <h2 class=\"list-item\">\n",
    "            header_element = row.find('h2', class_='list-item')\n",
    "            header_text = header_element.text.strip() if header_element else 'N/A'\n",
    "            \n",
    "            # Description/Summary: <p class=\"list-item\">\n",
    "            summary_element = row.find('p', class_='list-item')\n",
    "            summary = summary_element.text.strip() if summary_element else 'N/A'\n",
    "            \n",
    "            # Associated Link: <ul><li><a href=\"...\" class=\"list-item\">\n",
    "            link_element = row.find('a', class_='list-item')\n",
    "            \n",
    "            link_title = link_element.text.strip() if link_element else 'N/A'\n",
    "            link_url = link_element['href'] if link_element else 'N/A'\n",
    "            \n",
    "            # Prepend the base URL if the link is relative\n",
    "            if link_url != 'N/A' and link_url.startswith('/'):\n",
    "                link_url = 'https://fraser.stlouisfed.org' + link_url\n",
    "            \n",
    "            # Split the header into Date and Source\n",
    "            if '|' in header_text:\n",
    "                date, source = [part.strip() for part in header_text.split('|', 1)]\n",
    "            else:\n",
    "                date = header_text\n",
    "                source = 'N/A'\n",
    "\n",
    "            data.append({\n",
    "                'Date': date,\n",
    "                'Source': source,\n",
    "                'Summary': summary,\n",
    "                'Associated Link Title': link_title,\n",
    "                'Associated Link URL': link_url\n",
    "            })\n",
    "\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during the request: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Execution ---\n",
    "URL = \"https://fraser.stlouisfed.org/timeline/financial-crisis\"\n",
    "fraser_timeline_data = scrape_financial_crisis_timeline(URL)\n",
    "\n",
    "# Output the results (first 5 entries)\n",
    "if fraser_timeline_data:\n",
    "    df = pd.DataFrame(fraser_timeline_data)\n",
    "    print(f\"Scraped {len(df)} events.\")\n",
    "    print(\"\\n--- First 5 Scraped Events ---\")\n",
    "    print(df.head().to_markdown(index=False))\n",
    "else:\n",
    "    print(\"Failed to scrape data.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
