{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f52095",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f52d72",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.text_splitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Data Ingestion\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter  \u001b[38;5;66;03m# For splitting text into manageable chunks\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Tokenization & Embeddings\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.text_splitter'"
     ]
    }
   ],
   "source": [
    "# Data Ingestion\n",
    "import feedparser\n",
    "import newspaper\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Tokenization & Embeddings\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS  # For storing and retrieving embeddings using the FAISS library\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import joblib # For saving models\n",
    "\n",
    "# Cluster Interpretation\n",
    "\n",
    "# Identifier training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Clustifier training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e0fca4",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289454d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_full_articles_from_rss(rss_url, max_articles=50):\n",
    "    \"\"\"\n",
    "    Scrapes a given RSS feed, extracts article links, and uses newspaper3k\n",
    "    to download and parse the full text of each article.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting scrape for RSS: {rss_url} ---\")\n",
    "    \n",
    "    feed = feedparser.parse(rss_url)\n",
    "    articles_data = []\n",
    "    \n",
    "    # Iterate through entries, limiting the number of articles\n",
    "    for i, entry in enumerate(feed.entries):\n",
    "        if i >= max_articles:\n",
    "            print(f\"Reached max_articles limit of {max_articles}.\")\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            # 1. Extract link and basic metadata from RSS entry\n",
    "            link = entry.link\n",
    "            title = entry.title\n",
    "            \n",
    "            # Use feedparser's published_parsed and format to ISO standard\n",
    "            published_date = datetime.fromtimestamp(time.mktime(entry.published_parsed)).isoformat()\n",
    "            \n",
    "            # 2. Use newspaper3k to get the full article text\n",
    "            article = newspaper.Article(link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            \n",
    "            # Only proceed if we successfully parsed text\n",
    "            if article.text:\n",
    "                articles_data.append({\n",
    "                    'source': 'ECB',\n",
    "                    'title': title,\n",
    "                    'link': link,\n",
    "                    'date': published_date,\n",
    "                    'full_text': article.text\n",
    "                })\n",
    "                print(f\"Successfully scraped: {title[:50]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Skip if an article link is broken or parsing fails\n",
    "            print(f\"Error scraping article at {entry.link}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(articles_data)\n",
    "\n",
    "# --- Central Bank RSS Feeds (You will need to verify and add more) ---\n",
    "# NOTE: The ECB Press Releases feed link is typically very stable.\n",
    "ECB_RSS_URL = \"https://www.ecb.europa.eu/press/pr/date/html/index.rss\" \n",
    "# FED, BoE, and BoJ links should be found similarly for press releases/statements\n",
    "\n",
    "# --- EXECUTION ---\n",
    "ecb_df = scrape_full_articles_from_rss(ECB_RSS_URL, max_articles=10)\n",
    "# print(ecb_df.head())\n",
    "# print(f\"\\nTotal articles scraped: {len(ecb_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3b49c",
   "metadata": {},
   "source": [
    "Tokenization & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b298cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A specific model for generating embeddings, distinct from the sentiment classifier\n",
    "EMBEDDING_MODEL_NAME = \"ProsusAI/finbert\" \n",
    "\n",
    "def get_finbert_sentence_embeddings(texts, model_name=EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Loads FinBERT, tokenizes texts, and extracts the [CLS] token embedding \n",
    "    as the sentence representation.\n",
    "    \"\"\"\n",
    "    # 1. Load Tokenizer and Model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Using AutoModel for embeddings (not AutoModelForSequenceClassification, which is for sentiment classification)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Check for GPU and move model if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\n--- Generating FinBERT Embeddings on {device} ---\")\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # 2. Tokenize the text\n",
    "        # BERT models have a max sequence length (typically 512). \n",
    "        # The 'full_text' needs truncation or splitting (advanced step).\n",
    "        inputs = tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 3. Get model outputs\n",
    "            outputs = model(**inputs)\n",
    "            # The last hidden state contains the final embeddings for all tokens\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            \n",
    "            # 4. Extract the [CLS] token vector as the sentence embedding\n",
    "            # [CLS] token is at index 0, and we squeeze to remove the batch dimension\n",
    "            cls_embedding = last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(cls_embedding)\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# if not Text DataFrame is empty:\n",
    "    # Use the full_text column from the DataFrame created in Step 1\n",
    "    #texts_to_embed = Text DF.tolist()\n",
    "    \n",
    "    # Get the 768-dimensional embeddings\n",
    "    finbert_embeddings = get_finbert_sentence_embeddings(texts_to_embed)\n",
    "\n",
    "    # Add embeddings to the DataFrame for the next pipeline step\n",
    "    #Text DF['finbert_embedding'] = list(finbert_embeddings)\n",
    "\n",
    "    print(\"\\n--- Embeddings Generated ---\")\n",
    "    print(f\"Shape of Embeddings: {finbert_embeddings.shape}\")\n",
    "    print(\"DataFrame with embeddings ready for PCA/Clustering.\")\n",
    "else:\n",
    "    print(\"No articles to process. Please check data acquisition step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be012d8",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b151e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "def unsupervised_pipeline(embeddings, n_components=100, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Performs scaling, PCA for dimension reduction, and GMM for clustering.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Unsupervised Pipeline (PCA & GMM) ---\")\n",
    "\n",
    "    # 1. Scaling (Important for PCA/Clustering algorithms)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    print(f\"Scaled Embeddings Shape: {scaled_embeddings.shape}\")\n",
    "\n",
    "    # 2. Dimensionality Reduction (PCA)\n",
    "    # Target: Reduce from 768-dim to a more manageable size (e.g., 100)\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    reduced_data = pca.fit_transform(scaled_embeddings)\n",
    "    print(f\"PCA Reduced Data Shape: {reduced_data.shape}\")\n",
    "    \n",
    "    # Optional: Save PCA model for interpretation/reproducibility\n",
    "    joblib.dump(pca, 'pca_model.pkl')\n",
    "\n",
    "    # 3. Clustering (Gaussian Mixture Model - GMM)\n",
    "    # The number of clusters (n_clusters) should be determined via elbow method,\n",
    "    # silhouette score, or business logic.\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=42, covariance_type='full')\n",
    "    gmm_labels = gmm.fit_predict(reduced_data)\n",
    "    \n",
    "    print(f\"Clustering Complete. Found {n_clusters} clusters.\")\n",
    "    \n",
    "    return gmm_labels, gmm, reduced_data, pca\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "# Check if the embeddings were created\n",
    "if 'finbert_embedding' in ecb_df.columns and not ecb_df.empty:\n",
    "    \n",
    "    # Stack the list of embeddings into a single NumPy array\n",
    "    embedding_matrix = np.stack(ecb_df['finbert_embedding'].values)\n",
    "    \n",
    "    # Run the unsupervised steps\n",
    "    cluster_labels, gmm_model, reduced_data, pca_model = unsupervised_pipeline(\n",
    "        embedding_matrix, \n",
    "        n_components=100, \n",
    "        n_clusters=5 # Example value\n",
    "    )\n",
    "    \n",
    "    # Add the cluster label back to the original DataFrame\n",
    "    ecb_df['cluster_label'] = cluster_labels\n",
    "    \n",
    "    # Display cluster distribution\n",
    "    print(\"\\nCluster Distribution:\")\n",
    "    print(ecb_df['cluster_label'].value_counts())\n",
    "    print(\"\\nYour data is now clustered and ready for the interpretation (xAI) and the final supervised classification step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a138c05",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "# probs = gmm.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18bb581",
   "metadata": {},
   "source": [
    "Cluster Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93128136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a87da9",
   "metadata": {},
   "source": [
    "Identifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "# Select features for the model\n",
    "features = ['rooms', \n",
    "            'area', \n",
    "            'luxurious', \n",
    "            'pop_dens', \n",
    "            'mean_taxable_income', \n",
    "            'dist_supermarket']\n",
    "X = df[features]\n",
    "y = df['expensive']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Display the model coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': model.coef_[0]\n",
    "})\n",
    "coefficients\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e04c61",
   "metadata": {},
   "source": [
    "Clustifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da316b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Footer\n",
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
